{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Charottama Oshmar D. 6599126\n",
    "#Zhixing Yang 5524726\n",
    "#Christophorus Ivan Darmasaputra 5699551"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#importing all the necessary modules\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, LSTM, BatchNormalization\n",
    "from keras.callbacks import TensorBoard\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras import optimizers\n",
    "from keras import regularizers\n",
    "from keras.utils.np_utils import to_categorical\n",
    "\n",
    "#from tensorflow.keras.models import Sequential\n",
    "#from tensorflow.keras.layers import Conv1D, MaxPool1D, Flatten\n",
    "#from tensorflow.keras.layers import Dense, Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading the dataset using pandas\n",
    "missing_values = [\"n/a\", \"na\", \"--\", \"\"]\n",
    "\n",
    "abalone = pd.read_csv(\"/Users/charottamaoshmar/Desktop/Dataset-20191002/abaloneData.txt\", na_values = missing_values)\n",
    "abalone.columns=['sex', 'length', 'diameter', 'height', 'w1', 'w2', 'w3', 'w4', 'rings']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sex</th>\n",
       "      <th>length</th>\n",
       "      <th>diameter</th>\n",
       "      <th>height</th>\n",
       "      <th>w1</th>\n",
       "      <th>w2</th>\n",
       "      <th>w3</th>\n",
       "      <th>w4</th>\n",
       "      <th>rings</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>M</td>\n",
       "      <td>0.350</td>\n",
       "      <td>0.265</td>\n",
       "      <td>0.090</td>\n",
       "      <td>0.2255</td>\n",
       "      <td>0.0995</td>\n",
       "      <td>0.0485</td>\n",
       "      <td>0.070</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>F</td>\n",
       "      <td>0.530</td>\n",
       "      <td>0.420</td>\n",
       "      <td>0.135</td>\n",
       "      <td>0.6770</td>\n",
       "      <td>0.2565</td>\n",
       "      <td>0.1415</td>\n",
       "      <td>0.210</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>M</td>\n",
       "      <td>0.440</td>\n",
       "      <td>0.365</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.5160</td>\n",
       "      <td>0.2155</td>\n",
       "      <td>0.1140</td>\n",
       "      <td>0.155</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I</td>\n",
       "      <td>0.330</td>\n",
       "      <td>0.255</td>\n",
       "      <td>0.080</td>\n",
       "      <td>0.2050</td>\n",
       "      <td>0.0895</td>\n",
       "      <td>0.0395</td>\n",
       "      <td>0.055</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I</td>\n",
       "      <td>0.425</td>\n",
       "      <td>0.300</td>\n",
       "      <td>0.095</td>\n",
       "      <td>0.3515</td>\n",
       "      <td>0.1410</td>\n",
       "      <td>0.0775</td>\n",
       "      <td>0.120</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  sex  length  diameter  height      w1      w2      w3     w4  rings\n",
       "0   M   0.350     0.265   0.090  0.2255  0.0995  0.0485  0.070      7\n",
       "1   F   0.530     0.420   0.135  0.6770  0.2565  0.1415  0.210      9\n",
       "2   M   0.440     0.365   0.125  0.5160  0.2155  0.1140  0.155     10\n",
       "3   I   0.330     0.255   0.080  0.2050  0.0895  0.0395  0.055      7\n",
       "4   I   0.425     0.300   0.095  0.3515  0.1410  0.0775  0.120      8"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#checking if dataset is loaded properly\n",
    "abalone.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sex         0\n",
       "length      0\n",
       "diameter    0\n",
       "height      0\n",
       "w1          0\n",
       "w2          0\n",
       "w3          0\n",
       "w4          0\n",
       "rings       0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#part of preprocessing, finding out null/missing values\n",
    "abalone.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4176, 9)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#checking shape of dataset\n",
    "abalone.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 7,  9, 10,  8, 20, 16, 19, 14, 11, 12, 15, 18, 13,  5,  4,  6, 21,\n",
       "       17, 22,  1,  3, 26, 23, 29,  2, 27, 25, 24])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#finding out the unique values in 'rings'\n",
    "#this is a regression problem\n",
    "abalone['rings'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sex          object\n",
       "length      float64\n",
       "diameter    float64\n",
       "height      float64\n",
       "w1          float64\n",
       "w2          float64\n",
       "w3          float64\n",
       "w4          float64\n",
       "rings         int64\n",
       "dtype: object"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#checking datatypes of each column. Notice that 'sex' \n",
    "#is an object dtype column and we need to convert that\n",
    "abalone.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#making the dictionary and replacing the values inside 'sex' column\n",
    "sex_replace = ({'sex': {'M':0, 'F':1, 'I':2}})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "abalone = abalone.replace(sex_replace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#one hot encoding for the values we assigned in 'sex'\n",
    "\n",
    "from keras.utils.np_utils import to_categorical\n",
    "\n",
    "abalone['sex'] = to_categorical(abalone.sex, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sex         float32\n",
       "length      float64\n",
       "diameter    float64\n",
       "height      float64\n",
       "w1          float64\n",
       "w2          float64\n",
       "w3          float64\n",
       "w4          float64\n",
       "rings         int64\n",
       "dtype: object"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#now 'sex' is a float datatype so we can feed to the ML algorithm\n",
    "abalone.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sex</th>\n",
       "      <th>length</th>\n",
       "      <th>diameter</th>\n",
       "      <th>height</th>\n",
       "      <th>w1</th>\n",
       "      <th>w2</th>\n",
       "      <th>w3</th>\n",
       "      <th>w4</th>\n",
       "      <th>rings</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.350</td>\n",
       "      <td>0.265</td>\n",
       "      <td>0.090</td>\n",
       "      <td>0.2255</td>\n",
       "      <td>0.0995</td>\n",
       "      <td>0.0485</td>\n",
       "      <td>0.070</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.530</td>\n",
       "      <td>0.420</td>\n",
       "      <td>0.135</td>\n",
       "      <td>0.6770</td>\n",
       "      <td>0.2565</td>\n",
       "      <td>0.1415</td>\n",
       "      <td>0.210</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.440</td>\n",
       "      <td>0.365</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.5160</td>\n",
       "      <td>0.2155</td>\n",
       "      <td>0.1140</td>\n",
       "      <td>0.155</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.330</td>\n",
       "      <td>0.255</td>\n",
       "      <td>0.080</td>\n",
       "      <td>0.2050</td>\n",
       "      <td>0.0895</td>\n",
       "      <td>0.0395</td>\n",
       "      <td>0.055</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.425</td>\n",
       "      <td>0.300</td>\n",
       "      <td>0.095</td>\n",
       "      <td>0.3515</td>\n",
       "      <td>0.1410</td>\n",
       "      <td>0.0775</td>\n",
       "      <td>0.120</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sex  length  diameter  height      w1      w2      w3     w4  rings\n",
       "0  1.0   0.350     0.265   0.090  0.2255  0.0995  0.0485  0.070      7\n",
       "1  0.0   0.530     0.420   0.135  0.6770  0.2565  0.1415  0.210      9\n",
       "2  1.0   0.440     0.365   0.125  0.5160  0.2155  0.1140  0.155     10\n",
       "3  0.0   0.330     0.255   0.080  0.2050  0.0895  0.0395  0.055      7\n",
       "4  0.0   0.425     0.300   0.095  0.3515  0.1410  0.0775  0.120      8"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#rechecking dataset after replacement\n",
    "abalone.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#shuffling rows in the dataset for the sampling\n",
    "abalone = abalone.sample(frac=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#assigning columns to x and y. x is features, y is the target (label)\n",
    "x = abalone.iloc[:, 0:8]\n",
    "y = abalone.iloc[:, 8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sex</th>\n",
       "      <th>length</th>\n",
       "      <th>diameter</th>\n",
       "      <th>height</th>\n",
       "      <th>w1</th>\n",
       "      <th>w2</th>\n",
       "      <th>w3</th>\n",
       "      <th>w4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.350</td>\n",
       "      <td>0.265</td>\n",
       "      <td>0.090</td>\n",
       "      <td>0.2255</td>\n",
       "      <td>0.0995</td>\n",
       "      <td>0.0485</td>\n",
       "      <td>0.070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.530</td>\n",
       "      <td>0.420</td>\n",
       "      <td>0.135</td>\n",
       "      <td>0.6770</td>\n",
       "      <td>0.2565</td>\n",
       "      <td>0.1415</td>\n",
       "      <td>0.210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.440</td>\n",
       "      <td>0.365</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.5160</td>\n",
       "      <td>0.2155</td>\n",
       "      <td>0.1140</td>\n",
       "      <td>0.155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.330</td>\n",
       "      <td>0.255</td>\n",
       "      <td>0.080</td>\n",
       "      <td>0.2050</td>\n",
       "      <td>0.0895</td>\n",
       "      <td>0.0395</td>\n",
       "      <td>0.055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.425</td>\n",
       "      <td>0.300</td>\n",
       "      <td>0.095</td>\n",
       "      <td>0.3515</td>\n",
       "      <td>0.1410</td>\n",
       "      <td>0.0775</td>\n",
       "      <td>0.120</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sex  length  diameter  height      w1      w2      w3     w4\n",
       "0  1.0   0.350     0.265   0.090  0.2255  0.0995  0.0485  0.070\n",
       "1  0.0   0.530     0.420   0.135  0.6770  0.2565  0.1415  0.210\n",
       "2  1.0   0.440     0.365   0.125  0.5160  0.2155  0.1140  0.155\n",
       "3  0.0   0.330     0.255   0.080  0.2050  0.0895  0.0395  0.055\n",
       "4  0.0   0.425     0.300   0.095  0.3515  0.1410  0.0775  0.120"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#showing x (features) of original dataset. x does\n",
    "#not have 'rings' column, and is shuffled\n",
    "#(compare to abalone.head() above)\n",
    "x.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x[:int((len(x)*0.7))]\n",
    "x_test = x[int((len(x)*0.7)):]\n",
    "y_train = y[:int((len(y)*0.7))]\n",
    "y_test = y[int((len(y)*0.7)):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sex</th>\n",
       "      <th>length</th>\n",
       "      <th>diameter</th>\n",
       "      <th>height</th>\n",
       "      <th>w1</th>\n",
       "      <th>w2</th>\n",
       "      <th>w3</th>\n",
       "      <th>w4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.350</td>\n",
       "      <td>0.265</td>\n",
       "      <td>0.090</td>\n",
       "      <td>0.2255</td>\n",
       "      <td>0.0995</td>\n",
       "      <td>0.0485</td>\n",
       "      <td>0.0700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.530</td>\n",
       "      <td>0.420</td>\n",
       "      <td>0.135</td>\n",
       "      <td>0.6770</td>\n",
       "      <td>0.2565</td>\n",
       "      <td>0.1415</td>\n",
       "      <td>0.2100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.440</td>\n",
       "      <td>0.365</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.5160</td>\n",
       "      <td>0.2155</td>\n",
       "      <td>0.1140</td>\n",
       "      <td>0.1550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.330</td>\n",
       "      <td>0.255</td>\n",
       "      <td>0.080</td>\n",
       "      <td>0.2050</td>\n",
       "      <td>0.0895</td>\n",
       "      <td>0.0395</td>\n",
       "      <td>0.0550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.425</td>\n",
       "      <td>0.300</td>\n",
       "      <td>0.095</td>\n",
       "      <td>0.3515</td>\n",
       "      <td>0.1410</td>\n",
       "      <td>0.0775</td>\n",
       "      <td>0.1200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.530</td>\n",
       "      <td>0.415</td>\n",
       "      <td>0.150</td>\n",
       "      <td>0.7775</td>\n",
       "      <td>0.2370</td>\n",
       "      <td>0.1415</td>\n",
       "      <td>0.3300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.545</td>\n",
       "      <td>0.425</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.7680</td>\n",
       "      <td>0.2940</td>\n",
       "      <td>0.1495</td>\n",
       "      <td>0.2600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.475</td>\n",
       "      <td>0.370</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.5095</td>\n",
       "      <td>0.2165</td>\n",
       "      <td>0.1125</td>\n",
       "      <td>0.1650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.550</td>\n",
       "      <td>0.440</td>\n",
       "      <td>0.150</td>\n",
       "      <td>0.8945</td>\n",
       "      <td>0.3145</td>\n",
       "      <td>0.1510</td>\n",
       "      <td>0.3200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.525</td>\n",
       "      <td>0.380</td>\n",
       "      <td>0.140</td>\n",
       "      <td>0.6065</td>\n",
       "      <td>0.1940</td>\n",
       "      <td>0.1475</td>\n",
       "      <td>0.2100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.430</td>\n",
       "      <td>0.350</td>\n",
       "      <td>0.110</td>\n",
       "      <td>0.4060</td>\n",
       "      <td>0.1675</td>\n",
       "      <td>0.0810</td>\n",
       "      <td>0.1350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.490</td>\n",
       "      <td>0.380</td>\n",
       "      <td>0.135</td>\n",
       "      <td>0.5415</td>\n",
       "      <td>0.2175</td>\n",
       "      <td>0.0950</td>\n",
       "      <td>0.1900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.535</td>\n",
       "      <td>0.405</td>\n",
       "      <td>0.145</td>\n",
       "      <td>0.6845</td>\n",
       "      <td>0.2725</td>\n",
       "      <td>0.1710</td>\n",
       "      <td>0.2050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.470</td>\n",
       "      <td>0.355</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.4755</td>\n",
       "      <td>0.1675</td>\n",
       "      <td>0.0805</td>\n",
       "      <td>0.1850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0.400</td>\n",
       "      <td>0.130</td>\n",
       "      <td>0.6645</td>\n",
       "      <td>0.2580</td>\n",
       "      <td>0.1330</td>\n",
       "      <td>0.2400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.355</td>\n",
       "      <td>0.280</td>\n",
       "      <td>0.085</td>\n",
       "      <td>0.2905</td>\n",
       "      <td>0.0950</td>\n",
       "      <td>0.0395</td>\n",
       "      <td>0.1150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.440</td>\n",
       "      <td>0.340</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.4510</td>\n",
       "      <td>0.1880</td>\n",
       "      <td>0.0870</td>\n",
       "      <td>0.1300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.365</td>\n",
       "      <td>0.295</td>\n",
       "      <td>0.080</td>\n",
       "      <td>0.2555</td>\n",
       "      <td>0.0970</td>\n",
       "      <td>0.0430</td>\n",
       "      <td>0.1000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.450</td>\n",
       "      <td>0.320</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.3810</td>\n",
       "      <td>0.1705</td>\n",
       "      <td>0.0750</td>\n",
       "      <td>0.1150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.355</td>\n",
       "      <td>0.280</td>\n",
       "      <td>0.095</td>\n",
       "      <td>0.2455</td>\n",
       "      <td>0.0955</td>\n",
       "      <td>0.0620</td>\n",
       "      <td>0.0750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.380</td>\n",
       "      <td>0.275</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.2255</td>\n",
       "      <td>0.0800</td>\n",
       "      <td>0.0490</td>\n",
       "      <td>0.0850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.565</td>\n",
       "      <td>0.440</td>\n",
       "      <td>0.155</td>\n",
       "      <td>0.9395</td>\n",
       "      <td>0.4275</td>\n",
       "      <td>0.2140</td>\n",
       "      <td>0.2700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.550</td>\n",
       "      <td>0.415</td>\n",
       "      <td>0.135</td>\n",
       "      <td>0.7635</td>\n",
       "      <td>0.3180</td>\n",
       "      <td>0.2100</td>\n",
       "      <td>0.2000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.615</td>\n",
       "      <td>0.480</td>\n",
       "      <td>0.165</td>\n",
       "      <td>1.1615</td>\n",
       "      <td>0.5130</td>\n",
       "      <td>0.3010</td>\n",
       "      <td>0.3050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.560</td>\n",
       "      <td>0.440</td>\n",
       "      <td>0.140</td>\n",
       "      <td>0.9285</td>\n",
       "      <td>0.3825</td>\n",
       "      <td>0.1880</td>\n",
       "      <td>0.3000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.580</td>\n",
       "      <td>0.450</td>\n",
       "      <td>0.185</td>\n",
       "      <td>0.9955</td>\n",
       "      <td>0.3945</td>\n",
       "      <td>0.2720</td>\n",
       "      <td>0.2850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.590</td>\n",
       "      <td>0.445</td>\n",
       "      <td>0.140</td>\n",
       "      <td>0.9310</td>\n",
       "      <td>0.3560</td>\n",
       "      <td>0.2340</td>\n",
       "      <td>0.2800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.605</td>\n",
       "      <td>0.475</td>\n",
       "      <td>0.180</td>\n",
       "      <td>0.9365</td>\n",
       "      <td>0.3940</td>\n",
       "      <td>0.2190</td>\n",
       "      <td>0.2950</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.575</td>\n",
       "      <td>0.425</td>\n",
       "      <td>0.140</td>\n",
       "      <td>0.8635</td>\n",
       "      <td>0.3930</td>\n",
       "      <td>0.2270</td>\n",
       "      <td>0.2000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.580</td>\n",
       "      <td>0.470</td>\n",
       "      <td>0.165</td>\n",
       "      <td>0.9975</td>\n",
       "      <td>0.3935</td>\n",
       "      <td>0.2420</td>\n",
       "      <td>0.3300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2893</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.540</td>\n",
       "      <td>0.425</td>\n",
       "      <td>0.130</td>\n",
       "      <td>0.8155</td>\n",
       "      <td>0.3675</td>\n",
       "      <td>0.1365</td>\n",
       "      <td>0.2460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2894</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.540</td>\n",
       "      <td>0.415</td>\n",
       "      <td>0.110</td>\n",
       "      <td>0.6190</td>\n",
       "      <td>0.2755</td>\n",
       "      <td>0.1500</td>\n",
       "      <td>0.1765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2895</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.545</td>\n",
       "      <td>0.430</td>\n",
       "      <td>0.130</td>\n",
       "      <td>0.7595</td>\n",
       "      <td>0.3580</td>\n",
       "      <td>0.1530</td>\n",
       "      <td>0.2055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2896</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.545</td>\n",
       "      <td>0.430</td>\n",
       "      <td>0.150</td>\n",
       "      <td>0.7420</td>\n",
       "      <td>0.3525</td>\n",
       "      <td>0.1580</td>\n",
       "      <td>0.2080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2897</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.550</td>\n",
       "      <td>0.435</td>\n",
       "      <td>0.165</td>\n",
       "      <td>0.8040</td>\n",
       "      <td>0.3400</td>\n",
       "      <td>0.1940</td>\n",
       "      <td>0.2440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2898</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.550</td>\n",
       "      <td>0.425</td>\n",
       "      <td>0.130</td>\n",
       "      <td>0.6640</td>\n",
       "      <td>0.2695</td>\n",
       "      <td>0.1630</td>\n",
       "      <td>0.2100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2899</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.550</td>\n",
       "      <td>0.435</td>\n",
       "      <td>0.140</td>\n",
       "      <td>0.7450</td>\n",
       "      <td>0.3470</td>\n",
       "      <td>0.1740</td>\n",
       "      <td>0.2265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2900</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.560</td>\n",
       "      <td>0.430</td>\n",
       "      <td>0.130</td>\n",
       "      <td>0.7280</td>\n",
       "      <td>0.3355</td>\n",
       "      <td>0.1435</td>\n",
       "      <td>0.2175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2901</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.560</td>\n",
       "      <td>0.435</td>\n",
       "      <td>0.130</td>\n",
       "      <td>0.7770</td>\n",
       "      <td>0.3540</td>\n",
       "      <td>0.1730</td>\n",
       "      <td>0.2220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2902</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.575</td>\n",
       "      <td>0.425</td>\n",
       "      <td>0.150</td>\n",
       "      <td>0.8765</td>\n",
       "      <td>0.4550</td>\n",
       "      <td>0.1800</td>\n",
       "      <td>0.2280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2903</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.575</td>\n",
       "      <td>0.455</td>\n",
       "      <td>0.160</td>\n",
       "      <td>0.9895</td>\n",
       "      <td>0.4950</td>\n",
       "      <td>0.1950</td>\n",
       "      <td>0.2460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2904</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.575</td>\n",
       "      <td>0.450</td>\n",
       "      <td>0.165</td>\n",
       "      <td>0.9655</td>\n",
       "      <td>0.4980</td>\n",
       "      <td>0.1900</td>\n",
       "      <td>0.2300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2905</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.580</td>\n",
       "      <td>0.465</td>\n",
       "      <td>0.150</td>\n",
       "      <td>0.9065</td>\n",
       "      <td>0.3710</td>\n",
       "      <td>0.1965</td>\n",
       "      <td>0.2900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2906</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.580</td>\n",
       "      <td>0.460</td>\n",
       "      <td>0.150</td>\n",
       "      <td>1.0490</td>\n",
       "      <td>0.5205</td>\n",
       "      <td>0.1935</td>\n",
       "      <td>0.3050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2907</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.580</td>\n",
       "      <td>0.450</td>\n",
       "      <td>0.170</td>\n",
       "      <td>0.9705</td>\n",
       "      <td>0.4615</td>\n",
       "      <td>0.2320</td>\n",
       "      <td>0.2480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2908</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.580</td>\n",
       "      <td>0.450</td>\n",
       "      <td>0.150</td>\n",
       "      <td>0.9200</td>\n",
       "      <td>0.3930</td>\n",
       "      <td>0.2120</td>\n",
       "      <td>0.2895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2909</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.580</td>\n",
       "      <td>0.445</td>\n",
       "      <td>0.150</td>\n",
       "      <td>0.9525</td>\n",
       "      <td>0.4315</td>\n",
       "      <td>0.1945</td>\n",
       "      <td>0.2870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2910</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.580</td>\n",
       "      <td>0.440</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.7855</td>\n",
       "      <td>0.3630</td>\n",
       "      <td>0.1955</td>\n",
       "      <td>0.1950</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2911</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.585</td>\n",
       "      <td>0.450</td>\n",
       "      <td>0.135</td>\n",
       "      <td>0.8550</td>\n",
       "      <td>0.3795</td>\n",
       "      <td>0.1870</td>\n",
       "      <td>0.2600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2912</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.590</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0.150</td>\n",
       "      <td>1.1420</td>\n",
       "      <td>0.4850</td>\n",
       "      <td>0.2650</td>\n",
       "      <td>0.3450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2913</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.590</td>\n",
       "      <td>0.460</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.7550</td>\n",
       "      <td>0.3340</td>\n",
       "      <td>0.1500</td>\n",
       "      <td>0.2380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2914</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.590</td>\n",
       "      <td>0.475</td>\n",
       "      <td>0.145</td>\n",
       "      <td>0.9745</td>\n",
       "      <td>0.4675</td>\n",
       "      <td>0.2070</td>\n",
       "      <td>0.2590</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2915</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.595</td>\n",
       "      <td>0.470</td>\n",
       "      <td>0.155</td>\n",
       "      <td>1.2015</td>\n",
       "      <td>0.4920</td>\n",
       "      <td>0.3865</td>\n",
       "      <td>0.2650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2916</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.595</td>\n",
       "      <td>0.460</td>\n",
       "      <td>0.170</td>\n",
       "      <td>1.1295</td>\n",
       "      <td>0.5700</td>\n",
       "      <td>0.2555</td>\n",
       "      <td>0.2650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2917</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.600</td>\n",
       "      <td>0.445</td>\n",
       "      <td>0.135</td>\n",
       "      <td>0.9205</td>\n",
       "      <td>0.4450</td>\n",
       "      <td>0.2035</td>\n",
       "      <td>0.2530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2918</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.600</td>\n",
       "      <td>0.480</td>\n",
       "      <td>0.170</td>\n",
       "      <td>1.0560</td>\n",
       "      <td>0.4575</td>\n",
       "      <td>0.2435</td>\n",
       "      <td>0.3135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2919</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.600</td>\n",
       "      <td>0.450</td>\n",
       "      <td>0.195</td>\n",
       "      <td>1.3400</td>\n",
       "      <td>0.6170</td>\n",
       "      <td>0.3255</td>\n",
       "      <td>0.3605</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2920</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.600</td>\n",
       "      <td>0.450</td>\n",
       "      <td>0.150</td>\n",
       "      <td>0.9625</td>\n",
       "      <td>0.4375</td>\n",
       "      <td>0.2225</td>\n",
       "      <td>0.2775</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2921</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.600</td>\n",
       "      <td>0.465</td>\n",
       "      <td>0.165</td>\n",
       "      <td>1.0475</td>\n",
       "      <td>0.4650</td>\n",
       "      <td>0.2345</td>\n",
       "      <td>0.3150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2922</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.605</td>\n",
       "      <td>0.495</td>\n",
       "      <td>0.170</td>\n",
       "      <td>1.0915</td>\n",
       "      <td>0.4365</td>\n",
       "      <td>0.2715</td>\n",
       "      <td>0.3350</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2923 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      sex  length  diameter  height      w1      w2      w3      w4\n",
       "0     1.0   0.350     0.265   0.090  0.2255  0.0995  0.0485  0.0700\n",
       "1     0.0   0.530     0.420   0.135  0.6770  0.2565  0.1415  0.2100\n",
       "2     1.0   0.440     0.365   0.125  0.5160  0.2155  0.1140  0.1550\n",
       "3     0.0   0.330     0.255   0.080  0.2050  0.0895  0.0395  0.0550\n",
       "4     0.0   0.425     0.300   0.095  0.3515  0.1410  0.0775  0.1200\n",
       "5     0.0   0.530     0.415   0.150  0.7775  0.2370  0.1415  0.3300\n",
       "6     0.0   0.545     0.425   0.125  0.7680  0.2940  0.1495  0.2600\n",
       "7     1.0   0.475     0.370   0.125  0.5095  0.2165  0.1125  0.1650\n",
       "8     0.0   0.550     0.440   0.150  0.8945  0.3145  0.1510  0.3200\n",
       "9     0.0   0.525     0.380   0.140  0.6065  0.1940  0.1475  0.2100\n",
       "10    1.0   0.430     0.350   0.110  0.4060  0.1675  0.0810  0.1350\n",
       "11    1.0   0.490     0.380   0.135  0.5415  0.2175  0.0950  0.1900\n",
       "12    0.0   0.535     0.405   0.145  0.6845  0.2725  0.1710  0.2050\n",
       "13    0.0   0.470     0.355   0.100  0.4755  0.1675  0.0805  0.1850\n",
       "14    1.0   0.500     0.400   0.130  0.6645  0.2580  0.1330  0.2400\n",
       "15    0.0   0.355     0.280   0.085  0.2905  0.0950  0.0395  0.1150\n",
       "16    0.0   0.440     0.340   0.100  0.4510  0.1880  0.0870  0.1300\n",
       "17    1.0   0.365     0.295   0.080  0.2555  0.0970  0.0430  0.1000\n",
       "18    1.0   0.450     0.320   0.100  0.3810  0.1705  0.0750  0.1150\n",
       "19    1.0   0.355     0.280   0.095  0.2455  0.0955  0.0620  0.0750\n",
       "20    0.0   0.380     0.275   0.100  0.2255  0.0800  0.0490  0.0850\n",
       "21    0.0   0.565     0.440   0.155  0.9395  0.4275  0.2140  0.2700\n",
       "22    0.0   0.550     0.415   0.135  0.7635  0.3180  0.2100  0.2000\n",
       "23    0.0   0.615     0.480   0.165  1.1615  0.5130  0.3010  0.3050\n",
       "24    0.0   0.560     0.440   0.140  0.9285  0.3825  0.1880  0.3000\n",
       "25    0.0   0.580     0.450   0.185  0.9955  0.3945  0.2720  0.2850\n",
       "26    1.0   0.590     0.445   0.140  0.9310  0.3560  0.2340  0.2800\n",
       "27    1.0   0.605     0.475   0.180  0.9365  0.3940  0.2190  0.2950\n",
       "28    1.0   0.575     0.425   0.140  0.8635  0.3930  0.2270  0.2000\n",
       "29    1.0   0.580     0.470   0.165  0.9975  0.3935  0.2420  0.3300\n",
       "...   ...     ...       ...     ...     ...     ...     ...     ...\n",
       "2893  0.0   0.540     0.425   0.130  0.8155  0.3675  0.1365  0.2460\n",
       "2894  0.0   0.540     0.415   0.110  0.6190  0.2755  0.1500  0.1765\n",
       "2895  0.0   0.545     0.430   0.130  0.7595  0.3580  0.1530  0.2055\n",
       "2896  0.0   0.545     0.430   0.150  0.7420  0.3525  0.1580  0.2080\n",
       "2897  0.0   0.550     0.435   0.165  0.8040  0.3400  0.1940  0.2440\n",
       "2898  0.0   0.550     0.425   0.130  0.6640  0.2695  0.1630  0.2100\n",
       "2899  0.0   0.550     0.435   0.140  0.7450  0.3470  0.1740  0.2265\n",
       "2900  0.0   0.560     0.430   0.130  0.7280  0.3355  0.1435  0.2175\n",
       "2901  0.0   0.560     0.435   0.130  0.7770  0.3540  0.1730  0.2220\n",
       "2902  0.0   0.575     0.425   0.150  0.8765  0.4550  0.1800  0.2280\n",
       "2903  0.0   0.575     0.455   0.160  0.9895  0.4950  0.1950  0.2460\n",
       "2904  1.0   0.575     0.450   0.165  0.9655  0.4980  0.1900  0.2300\n",
       "2905  1.0   0.580     0.465   0.150  0.9065  0.3710  0.1965  0.2900\n",
       "2906  1.0   0.580     0.460   0.150  1.0490  0.5205  0.1935  0.3050\n",
       "2907  0.0   0.580     0.450   0.170  0.9705  0.4615  0.2320  0.2480\n",
       "2908  0.0   0.580     0.450   0.150  0.9200  0.3930  0.2120  0.2895\n",
       "2909  1.0   0.580     0.445   0.150  0.9525  0.4315  0.1945  0.2870\n",
       "2910  0.0   0.580     0.440   0.125  0.7855  0.3630  0.1955  0.1950\n",
       "2911  0.0   0.585     0.450   0.135  0.8550  0.3795  0.1870  0.2600\n",
       "2912  1.0   0.590     0.500   0.150  1.1420  0.4850  0.2650  0.3450\n",
       "2913  0.0   0.590     0.460   0.125  0.7550  0.3340  0.1500  0.2380\n",
       "2914  0.0   0.590     0.475   0.145  0.9745  0.4675  0.2070  0.2590\n",
       "2915  1.0   0.595     0.470   0.155  1.2015  0.4920  0.3865  0.2650\n",
       "2916  1.0   0.595     0.460   0.170  1.1295  0.5700  0.2555  0.2650\n",
       "2917  0.0   0.600     0.445   0.135  0.9205  0.4450  0.2035  0.2530\n",
       "2918  0.0   0.600     0.480   0.170  1.0560  0.4575  0.2435  0.3135\n",
       "2919  1.0   0.600     0.450   0.195  1.3400  0.6170  0.3255  0.3605\n",
       "2920  0.0   0.600     0.450   0.150  0.9625  0.4375  0.2225  0.2775\n",
       "2921  1.0   0.600     0.465   0.165  1.0475  0.4650  0.2345  0.3150\n",
       "2922  0.0   0.605     0.495   0.170  1.0915  0.4365  0.2715  0.3350\n",
       "\n",
       "[2923 rows x 8 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2923, 8)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#chacking the shape after splitting into training and testing dataset\n",
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1253,)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#now let's build the model. Keras provides a Sequential() model to build\n",
    "#MLPs and ANNs. \n",
    "\n",
    "from keras import regularizers\n",
    "\n",
    "mlp_model = Sequential()\n",
    "\n",
    "#Dense(12) is the first hidden layer with 12 nodes which take 8 inputs,\n",
    "#being the number of features. Regularizer allow penalties to be incorporated in the loss function\n",
    "# We use the Rectified Linear Unit for the activation of the hidden layers\n",
    "mlp_model.add(Dense(12, input_dim=8, kernel_regularizer=regularizers.l2(0.01), activation='relu'))\n",
    "\n",
    "#Dropout(0.2) is a way to prevent overfitting by *dropping out* some of the nodes\n",
    "#of that layer. 0.2 is the dropout ratio (out of 1)\n",
    "mlp_model.add(Dropout(0.2))\n",
    "\n",
    "#Second hidden layer, containing 6 nodes\n",
    "mlp_model.add(Dense(6, activation='relu'))\n",
    "mlp_model.add(Dropout(0.2))\n",
    "\n",
    "#As a regression problem, the output is 1 because there is nothing to classify\n",
    "#from the result. Acttivation is set as None for this regression problem too\n",
    "mlp_model.add(Dense(1, activation='linear'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 12)                108       \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 12)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 6)                 78        \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 6)                 0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 7         \n",
      "=================================================================\n",
      "Total params: 193\n",
      "Trainable params: 193\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#Make a summary of the model we made. \n",
    "mlp_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Keras provides options to use for the loss and optimizer function.\n",
    "#In a Keras model, these are the hyperparameters that must be defined\n",
    "#To check the performance we use MSE (mean squared error)\n",
    "from keras import optimizers\n",
    "\n",
    "opt = optimizers.Adam(lr=0.1)\n",
    "\n",
    "mlp_model.compile(loss='mean_squared_error', optimizer=opt, metrics=['mse'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      " - 0s - loss: 25.2828 - mse: 25.1101\n",
      "Epoch 2/300\n",
      " - 0s - loss: 10.9210 - mse: 10.8054\n",
      "Epoch 3/300\n",
      " - 0s - loss: 9.0526 - mse: 8.9112\n",
      "Epoch 4/300\n",
      " - 0s - loss: 8.4914 - mse: 8.2758\n",
      "Epoch 5/300\n",
      " - 0s - loss: 8.7654 - mse: 8.4564\n",
      "Epoch 6/300\n",
      " - 0s - loss: 9.3747 - mse: 9.0676\n",
      "Epoch 7/300\n",
      " - 0s - loss: 8.9724 - mse: 8.6921\n",
      "Epoch 8/300\n",
      " - 0s - loss: 8.9804 - mse: 8.6984\n",
      "Epoch 9/300\n",
      " - 0s - loss: 8.3141 - mse: 8.0418\n",
      "Epoch 10/300\n",
      " - 0s - loss: 8.5963 - mse: 8.3301\n",
      "Epoch 11/300\n",
      " - 0s - loss: 8.8801 - mse: 8.6228\n",
      "Epoch 12/300\n",
      " - 0s - loss: 8.6570 - mse: 8.4028\n",
      "Epoch 13/300\n",
      " - 0s - loss: 8.5356 - mse: 8.2771\n",
      "Epoch 14/300\n",
      " - 0s - loss: 8.4343 - mse: 8.1528\n",
      "Epoch 15/300\n",
      " - 0s - loss: 8.1958 - mse: 7.9225\n",
      "Epoch 16/300\n",
      " - 0s - loss: 8.6078 - mse: 8.3607\n",
      "Epoch 17/300\n",
      " - 0s - loss: 8.4795 - mse: 8.2458\n",
      "Epoch 18/300\n",
      " - 0s - loss: 8.6412 - mse: 8.4108\n",
      "Epoch 19/300\n",
      " - 0s - loss: 8.5821 - mse: 8.3380\n",
      "Epoch 20/300\n",
      " - 0s - loss: 8.5393 - mse: 8.2924\n",
      "Epoch 21/300\n",
      " - 0s - loss: 8.9171 - mse: 8.6716\n",
      "Epoch 22/300\n",
      " - 0s - loss: 8.9198 - mse: 8.6678\n",
      "Epoch 23/300\n",
      " - 0s - loss: 9.0820 - mse: 8.8056\n",
      "Epoch 24/300\n",
      " - 0s - loss: 8.6533 - mse: 8.3762\n",
      "Epoch 25/300\n",
      " - 0s - loss: 8.7949 - mse: 8.5253\n",
      "Epoch 26/300\n",
      " - 0s - loss: 8.2094 - mse: 7.9391\n",
      "Epoch 27/300\n",
      " - 0s - loss: 8.5526 - mse: 8.2801\n",
      "Epoch 28/300\n",
      " - 0s - loss: 9.0107 - mse: 8.7325\n",
      "Epoch 29/300\n",
      " - 0s - loss: 8.8367 - mse: 8.5516\n",
      "Epoch 30/300\n",
      " - 0s - loss: 8.8171 - mse: 8.5304\n",
      "Epoch 31/300\n",
      " - 0s - loss: 8.3685 - mse: 8.0847\n",
      "Epoch 32/300\n",
      " - 0s - loss: 8.9008 - mse: 8.6197\n",
      "Epoch 33/300\n",
      " - 0s - loss: 8.5064 - mse: 8.2323\n",
      "Epoch 34/300\n",
      " - 0s - loss: 8.6558 - mse: 8.3956\n",
      "Epoch 35/300\n",
      " - 0s - loss: 8.5568 - mse: 8.2902\n",
      "Epoch 36/300\n",
      " - 0s - loss: 8.7080 - mse: 8.4470\n",
      "Epoch 37/300\n",
      " - 0s - loss: 8.4879 - mse: 8.2318\n",
      "Epoch 38/300\n",
      " - 0s - loss: 9.2428 - mse: 8.9880\n",
      "Epoch 39/300\n",
      " - 0s - loss: 8.8879 - mse: 8.6227\n",
      "Epoch 40/300\n",
      " - 0s - loss: 8.5093 - mse: 8.2539\n",
      "Epoch 41/300\n",
      " - 0s - loss: 8.6371 - mse: 8.3970\n",
      "Epoch 42/300\n",
      " - 0s - loss: 8.4596 - mse: 8.2394\n",
      "Epoch 43/300\n",
      " - 0s - loss: 8.6698 - mse: 8.4341\n",
      "Epoch 44/300\n",
      " - 0s - loss: 8.4675 - mse: 8.2300\n",
      "Epoch 45/300\n",
      " - 0s - loss: 8.9173 - mse: 8.6788\n",
      "Epoch 46/300\n",
      " - 0s - loss: 8.8512 - mse: 8.5925\n",
      "Epoch 47/300\n",
      " - 0s - loss: 8.7080 - mse: 8.4514\n",
      "Epoch 48/300\n",
      " - 0s - loss: 8.5509 - mse: 8.3130\n",
      "Epoch 49/300\n",
      " - 0s - loss: 8.8381 - mse: 8.6064\n",
      "Epoch 50/300\n",
      " - 0s - loss: 8.8033 - mse: 8.5599\n",
      "Epoch 51/300\n",
      " - 0s - loss: 8.4893 - mse: 8.2473\n",
      "Epoch 52/300\n",
      " - 0s - loss: 9.1729 - mse: 8.9241\n",
      "Epoch 53/300\n",
      " - 0s - loss: 8.6123 - mse: 8.3562\n",
      "Epoch 54/300\n",
      " - 0s - loss: 8.8677 - mse: 8.6136\n",
      "Epoch 55/300\n",
      " - 0s - loss: 8.9050 - mse: 8.6318\n",
      "Epoch 56/300\n",
      " - 0s - loss: 8.7136 - mse: 8.4425\n",
      "Epoch 57/300\n",
      " - 0s - loss: 8.5050 - mse: 8.2468\n",
      "Epoch 58/300\n",
      " - 0s - loss: 8.6809 - mse: 8.4301\n",
      "Epoch 59/300\n",
      " - 0s - loss: 8.6339 - mse: 8.4082\n",
      "Epoch 60/300\n",
      " - 0s - loss: 8.5517 - mse: 8.3175\n",
      "Epoch 61/300\n",
      " - 0s - loss: 8.6125 - mse: 8.3750\n",
      "Epoch 62/300\n",
      " - 0s - loss: 8.8342 - mse: 8.5782\n",
      "Epoch 63/300\n",
      " - 0s - loss: 8.3962 - mse: 8.1412\n",
      "Epoch 64/300\n",
      " - 0s - loss: 8.4912 - mse: 8.2433\n",
      "Epoch 65/300\n",
      " - 0s - loss: 8.8281 - mse: 8.5729\n",
      "Epoch 66/300\n",
      " - 0s - loss: 8.8401 - mse: 8.5923\n",
      "Epoch 67/300\n",
      " - 0s - loss: 8.7453 - mse: 8.5059\n",
      "Epoch 68/300\n",
      " - 0s - loss: 8.8522 - mse: 8.6131\n",
      "Epoch 69/300\n",
      " - 0s - loss: 8.5364 - mse: 8.2863\n",
      "Epoch 70/300\n",
      " - 0s - loss: 8.6238 - mse: 8.3768\n",
      "Epoch 71/300\n",
      " - 0s - loss: 8.6358 - mse: 8.4001\n",
      "Epoch 72/300\n",
      " - 0s - loss: 8.6757 - mse: 8.4412\n",
      "Epoch 73/300\n",
      " - 0s - loss: 8.4192 - mse: 8.1853\n",
      "Epoch 74/300\n",
      " - 0s - loss: 8.7769 - mse: 8.5490\n",
      "Epoch 75/300\n",
      " - 0s - loss: 8.5709 - mse: 8.3272\n",
      "Epoch 76/300\n",
      " - 0s - loss: 8.9758 - mse: 8.7315\n",
      "Epoch 77/300\n",
      " - 0s - loss: 8.9732 - mse: 8.7390\n",
      "Epoch 78/300\n",
      " - 0s - loss: 8.7838 - mse: 8.5465\n",
      "Epoch 79/300\n",
      " - 0s - loss: 9.0950 - mse: 8.8406\n",
      "Epoch 80/300\n",
      " - 0s - loss: 8.6398 - mse: 8.3938\n",
      "Epoch 81/300\n",
      " - 0s - loss: 8.6070 - mse: 8.3679\n",
      "Epoch 82/300\n",
      " - 0s - loss: 8.5573 - mse: 8.3255\n",
      "Epoch 83/300\n",
      " - 0s - loss: 8.4519 - mse: 8.2255\n",
      "Epoch 84/300\n",
      " - 0s - loss: 8.5245 - mse: 8.3007\n",
      "Epoch 85/300\n",
      " - 0s - loss: 8.8754 - mse: 8.6601\n",
      "Epoch 86/300\n",
      " - 0s - loss: 8.5348 - mse: 8.3177\n",
      "Epoch 87/300\n",
      " - 0s - loss: 9.0131 - mse: 8.7932\n",
      "Epoch 88/300\n",
      " - 0s - loss: 8.3950 - mse: 8.1585\n",
      "Epoch 89/300\n",
      " - 0s - loss: 8.5541 - mse: 8.3193\n",
      "Epoch 90/300\n",
      " - 0s - loss: 8.9076 - mse: 8.6748\n",
      "Epoch 91/300\n",
      " - 0s - loss: 8.7045 - mse: 8.4700\n",
      "Epoch 92/300\n",
      " - 0s - loss: 9.2308 - mse: 8.9843\n",
      "Epoch 93/300\n",
      " - 0s - loss: 8.9181 - mse: 8.6639\n",
      "Epoch 94/300\n",
      " - 0s - loss: 8.4566 - mse: 8.2109\n",
      "Epoch 95/300\n",
      " - 0s - loss: 8.5514 - mse: 8.3088\n",
      "Epoch 96/300\n",
      " - 0s - loss: 8.5221 - mse: 8.2906\n",
      "Epoch 97/300\n",
      " - 0s - loss: 9.0712 - mse: 8.8240\n",
      "Epoch 98/300\n",
      " - 0s - loss: 8.7574 - mse: 8.5142\n",
      "Epoch 99/300\n",
      " - 0s - loss: 8.6699 - mse: 8.4437\n",
      "Epoch 100/300\n",
      " - 0s - loss: 8.7170 - mse: 8.4833\n",
      "Epoch 101/300\n",
      " - 0s - loss: 8.9629 - mse: 8.7270\n",
      "Epoch 102/300\n",
      " - 0s - loss: 8.4295 - mse: 8.1940\n",
      "Epoch 103/300\n",
      " - 0s - loss: 8.3506 - mse: 8.1231\n",
      "Epoch 104/300\n",
      " - 0s - loss: 9.5099 - mse: 9.2869\n",
      "Epoch 105/300\n",
      " - 0s - loss: 8.9829 - mse: 8.7573\n",
      "Epoch 106/300\n",
      " - 0s - loss: 8.8614 - mse: 8.6329\n",
      "Epoch 107/300\n",
      " - 0s - loss: 8.5741 - mse: 8.3359\n",
      "Epoch 108/300\n",
      " - 0s - loss: 9.1711 - mse: 8.9336\n",
      "Epoch 109/300\n",
      " - 0s - loss: 8.4746 - mse: 8.2336\n",
      "Epoch 110/300\n",
      " - 0s - loss: 8.6913 - mse: 8.4406\n",
      "Epoch 111/300\n",
      " - 0s - loss: 8.9128 - mse: 8.6587\n",
      "Epoch 112/300\n",
      " - 0s - loss: 8.6483 - mse: 8.3864\n",
      "Epoch 113/300\n",
      " - 0s - loss: 8.6651 - mse: 8.4125\n",
      "Epoch 114/300\n",
      " - 0s - loss: 8.4314 - mse: 8.1827\n",
      "Epoch 115/300\n",
      " - 0s - loss: 8.3805 - mse: 8.1486\n",
      "Epoch 116/300\n",
      " - 0s - loss: 8.5544 - mse: 8.3330\n",
      "Epoch 117/300\n",
      " - 0s - loss: 10.2070 - mse: 9.9850\n",
      "Epoch 118/300\n",
      " - 0s - loss: 10.7105 - mse: 10.5235\n",
      "Epoch 119/300\n",
      " - 0s - loss: 10.6949 - mse: 10.5467\n",
      "Epoch 120/300\n",
      " - 0s - loss: 10.6368 - mse: 10.5202\n",
      "Epoch 121/300\n",
      " - 0s - loss: 10.6166 - mse: 10.5253\n",
      "Epoch 122/300\n",
      " - 0s - loss: 10.5966 - mse: 10.5257\n",
      "Epoch 123/300\n",
      " - 0s - loss: 10.5874 - mse: 10.5328\n",
      "Epoch 124/300\n",
      " - 0s - loss: 10.5821 - mse: 10.5402\n",
      "Epoch 125/300\n",
      " - 0s - loss: 10.5770 - mse: 10.5451\n",
      "Epoch 126/300\n",
      " - 0s - loss: 10.5428 - mse: 10.5187\n",
      "Epoch 127/300\n",
      " - 0s - loss: 10.5485 - mse: 10.5304\n",
      "Epoch 128/300\n",
      " - 0s - loss: 10.5637 - mse: 10.5501\n",
      "Epoch 129/300\n",
      " - 0s - loss: 10.5484 - mse: 10.5383\n",
      "Epoch 130/300\n",
      " - 0s - loss: 10.5283 - mse: 10.5208\n",
      "Epoch 131/300\n",
      " - 0s - loss: 10.5286 - mse: 10.5230\n",
      "Epoch 132/300\n",
      " - 0s - loss: 10.5407 - mse: 10.5366\n",
      "Epoch 133/300\n",
      " - 0s - loss: 10.5619 - mse: 10.5588\n",
      "Epoch 134/300\n",
      " - 0s - loss: 10.5504 - mse: 10.5481\n",
      "Epoch 135/300\n",
      " - 0s - loss: 10.5317 - mse: 10.5299\n",
      "Epoch 136/300\n",
      " - 0s - loss: 10.5484 - mse: 10.5471\n",
      "Epoch 137/300\n",
      " - 0s - loss: 10.5317 - mse: 10.5307\n",
      "Epoch 138/300\n",
      " - 0s - loss: 10.5316 - mse: 10.5308\n",
      "Epoch 139/300\n",
      " - 0s - loss: 10.5413 - mse: 10.5408\n",
      "Epoch 140/300\n",
      " - 0s - loss: 10.5457 - mse: 10.5453\n",
      "Epoch 141/300\n",
      " - 0s - loss: 10.5483 - mse: 10.5480\n",
      "Epoch 142/300\n",
      " - 0s - loss: 10.5369 - mse: 10.5366\n",
      "Epoch 143/300\n",
      " - 0s - loss: 10.5624 - mse: 10.5622\n",
      "Epoch 144/300\n",
      " - 0s - loss: 10.5196 - mse: 10.5195\n",
      "Epoch 145/300\n",
      " - 0s - loss: 10.5121 - mse: 10.5120\n",
      "Epoch 146/300\n",
      " - 0s - loss: 10.5487 - mse: 10.5486\n",
      "Epoch 147/300\n",
      " - 0s - loss: 10.5347 - mse: 10.5346\n",
      "Epoch 148/300\n",
      " - 0s - loss: 10.5106 - mse: 10.5106\n",
      "Epoch 149/300\n",
      " - 0s - loss: 10.5306 - mse: 10.5306\n",
      "Epoch 150/300\n",
      " - 0s - loss: 10.5092 - mse: 10.5092\n",
      "Epoch 151/300\n",
      " - 0s - loss: 10.5492 - mse: 10.5492\n",
      "Epoch 152/300\n",
      " - 0s - loss: 10.5432 - mse: 10.5432\n",
      "Epoch 153/300\n",
      " - 0s - loss: 10.5284 - mse: 10.5284\n",
      "Epoch 154/300\n",
      " - 0s - loss: 10.5376 - mse: 10.5376\n",
      "Epoch 155/300\n",
      " - 0s - loss: 10.5389 - mse: 10.5389\n",
      "Epoch 156/300\n",
      " - 0s - loss: 10.5391 - mse: 10.5391\n",
      "Epoch 157/300\n",
      " - 0s - loss: 10.5160 - mse: 10.5160\n",
      "Epoch 158/300\n",
      " - 0s - loss: 10.5367 - mse: 10.5367\n",
      "Epoch 159/300\n",
      " - 0s - loss: 10.5525 - mse: 10.5525\n",
      "Epoch 160/300\n",
      " - 0s - loss: 10.5180 - mse: 10.5180\n",
      "Epoch 161/300\n",
      " - 0s - loss: 10.5187 - mse: 10.5187\n",
      "Epoch 162/300\n",
      " - 0s - loss: 10.5201 - mse: 10.5201\n",
      "Epoch 163/300\n",
      " - 0s - loss: 10.5437 - mse: 10.5437\n",
      "Epoch 164/300\n",
      " - 0s - loss: 10.5388 - mse: 10.5388\n",
      "Epoch 165/300\n",
      " - 0s - loss: 10.5203 - mse: 10.5203\n",
      "Epoch 166/300\n",
      " - 0s - loss: 10.5308 - mse: 10.5308\n",
      "Epoch 167/300\n",
      " - 0s - loss: 10.5218 - mse: 10.5218\n",
      "Epoch 168/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - 0s - loss: 10.5545 - mse: 10.5545\n",
      "Epoch 169/300\n",
      " - 0s - loss: 10.5254 - mse: 10.5254\n",
      "Epoch 170/300\n",
      " - 0s - loss: 10.5311 - mse: 10.5311\n",
      "Epoch 171/300\n",
      " - 0s - loss: 10.5353 - mse: 10.5353\n",
      "Epoch 172/300\n",
      " - 0s - loss: 10.5253 - mse: 10.5253\n",
      "Epoch 173/300\n",
      " - 0s - loss: 10.5217 - mse: 10.5217\n",
      "Epoch 174/300\n",
      " - 0s - loss: 10.5221 - mse: 10.5221\n",
      "Epoch 175/300\n",
      " - 0s - loss: 10.5375 - mse: 10.5375\n",
      "Epoch 176/300\n",
      " - 0s - loss: 10.5406 - mse: 10.5406\n",
      "Epoch 177/300\n",
      " - 0s - loss: 10.5554 - mse: 10.5554\n",
      "Epoch 178/300\n",
      " - 0s - loss: 10.5200 - mse: 10.5200\n",
      "Epoch 179/300\n",
      " - 0s - loss: 10.5538 - mse: 10.5538\n",
      "Epoch 180/300\n",
      " - 0s - loss: 10.5578 - mse: 10.5578\n",
      "Epoch 181/300\n",
      " - 0s - loss: 10.5359 - mse: 10.5359\n",
      "Epoch 182/300\n",
      " - 0s - loss: 10.5189 - mse: 10.5189\n",
      "Epoch 183/300\n",
      " - 0s - loss: 10.5563 - mse: 10.5563\n",
      "Epoch 184/300\n",
      " - 0s - loss: 10.5445 - mse: 10.5445\n",
      "Epoch 185/300\n",
      " - 0s - loss: 10.5358 - mse: 10.5358\n",
      "Epoch 186/300\n",
      " - 0s - loss: 10.5295 - mse: 10.5295\n",
      "Epoch 187/300\n",
      " - 0s - loss: 10.5445 - mse: 10.5445\n",
      "Epoch 188/300\n",
      " - 0s - loss: 10.5941 - mse: 10.5941\n",
      "Epoch 189/300\n",
      " - 0s - loss: 10.5318 - mse: 10.5318\n",
      "Epoch 190/300\n",
      " - 0s - loss: 10.5398 - mse: 10.5398\n",
      "Epoch 191/300\n",
      " - 0s - loss: 10.5258 - mse: 10.5258\n",
      "Epoch 192/300\n",
      " - 0s - loss: 10.5502 - mse: 10.5502\n",
      "Epoch 193/300\n",
      " - 0s - loss: 10.5398 - mse: 10.5398\n",
      "Epoch 194/300\n",
      " - 0s - loss: 10.5709 - mse: 10.5709\n",
      "Epoch 195/300\n",
      " - 0s - loss: 10.5519 - mse: 10.5519\n",
      "Epoch 196/300\n",
      " - 0s - loss: 10.5777 - mse: 10.5777\n",
      "Epoch 197/300\n",
      " - 0s - loss: 10.5463 - mse: 10.5463\n",
      "Epoch 198/300\n",
      " - 0s - loss: 10.5329 - mse: 10.5329\n",
      "Epoch 199/300\n",
      " - 0s - loss: 10.5632 - mse: 10.5632\n",
      "Epoch 200/300\n",
      " - 0s - loss: 10.5369 - mse: 10.5369\n",
      "Epoch 201/300\n",
      " - 0s - loss: 10.5574 - mse: 10.5574\n",
      "Epoch 202/300\n",
      " - 0s - loss: 10.5380 - mse: 10.5380\n",
      "Epoch 203/300\n",
      " - 0s - loss: 10.5276 - mse: 10.5276\n",
      "Epoch 204/300\n",
      " - 0s - loss: 10.5656 - mse: 10.5656\n",
      "Epoch 205/300\n",
      " - 0s - loss: 10.5176 - mse: 10.5176\n",
      "Epoch 206/300\n",
      " - 0s - loss: 10.5348 - mse: 10.5348\n",
      "Epoch 207/300\n",
      " - 0s - loss: 10.5367 - mse: 10.5368\n",
      "Epoch 208/300\n",
      " - 0s - loss: 10.5225 - mse: 10.5225\n",
      "Epoch 209/300\n",
      " - 0s - loss: 10.5270 - mse: 10.5270\n",
      "Epoch 210/300\n",
      " - 0s - loss: 10.5322 - mse: 10.5322\n",
      "Epoch 211/300\n",
      " - 0s - loss: 10.5740 - mse: 10.5740\n",
      "Epoch 212/300\n",
      " - 0s - loss: 10.5560 - mse: 10.5560\n",
      "Epoch 213/300\n",
      " - 0s - loss: 10.5310 - mse: 10.5310\n",
      "Epoch 214/300\n",
      " - 0s - loss: 10.5357 - mse: 10.5357\n",
      "Epoch 215/300\n",
      " - 0s - loss: 10.5377 - mse: 10.5377\n",
      "Epoch 216/300\n",
      " - 0s - loss: 10.5432 - mse: 10.5432\n",
      "Epoch 217/300\n",
      " - 0s - loss: 10.5422 - mse: 10.5422\n",
      "Epoch 218/300\n",
      " - 0s - loss: 10.5117 - mse: 10.5117\n",
      "Epoch 219/300\n",
      " - 0s - loss: 10.5253 - mse: 10.5253\n",
      "Epoch 220/300\n",
      " - 0s - loss: 10.5094 - mse: 10.5094\n",
      "Epoch 221/300\n",
      " - 0s - loss: 10.5534 - mse: 10.5534\n",
      "Epoch 222/300\n",
      " - 0s - loss: 10.5362 - mse: 10.5362\n",
      "Epoch 223/300\n",
      " - 0s - loss: 10.5554 - mse: 10.5554\n",
      "Epoch 224/300\n",
      " - 0s - loss: 10.5345 - mse: 10.5345\n",
      "Epoch 225/300\n",
      " - 0s - loss: 10.5221 - mse: 10.5221\n",
      "Epoch 226/300\n",
      " - 0s - loss: 10.5748 - mse: 10.5748\n",
      "Epoch 227/300\n",
      " - 0s - loss: 10.5233 - mse: 10.5233\n",
      "Epoch 228/300\n",
      " - 0s - loss: 10.5611 - mse: 10.5611\n",
      "Epoch 229/300\n",
      " - 0s - loss: 10.5319 - mse: 10.5319\n",
      "Epoch 230/300\n",
      " - 0s - loss: 10.5819 - mse: 10.5819\n",
      "Epoch 231/300\n",
      " - 0s - loss: 10.5168 - mse: 10.5168\n",
      "Epoch 232/300\n",
      " - 0s - loss: 10.5376 - mse: 10.5376\n",
      "Epoch 233/300\n",
      " - 0s - loss: 10.5394 - mse: 10.5394\n",
      "Epoch 234/300\n",
      " - 0s - loss: 10.5324 - mse: 10.5324\n",
      "Epoch 235/300\n",
      " - 0s - loss: 10.5271 - mse: 10.5271\n",
      "Epoch 236/300\n",
      " - 0s - loss: 10.5533 - mse: 10.5533\n",
      "Epoch 237/300\n",
      " - 0s - loss: 10.5478 - mse: 10.5478\n",
      "Epoch 238/300\n",
      " - 0s - loss: 10.5403 - mse: 10.5403\n",
      "Epoch 239/300\n",
      " - 0s - loss: 10.5303 - mse: 10.5303\n",
      "Epoch 240/300\n",
      " - 0s - loss: 10.5538 - mse: 10.5538\n",
      "Epoch 241/300\n",
      " - 0s - loss: 10.5860 - mse: 10.5860\n",
      "Epoch 242/300\n",
      " - 0s - loss: 10.5194 - mse: 10.5195\n",
      "Epoch 243/300\n",
      " - 0s - loss: 10.5185 - mse: 10.5185\n",
      "Epoch 244/300\n",
      " - 0s - loss: 10.5246 - mse: 10.5246\n",
      "Epoch 245/300\n",
      " - 0s - loss: 10.5179 - mse: 10.5179\n",
      "Epoch 246/300\n",
      " - 0s - loss: 10.5371 - mse: 10.5371\n",
      "Epoch 247/300\n",
      " - 0s - loss: 10.5502 - mse: 10.5502\n",
      "Epoch 248/300\n",
      " - 0s - loss: 10.5564 - mse: 10.5564\n",
      "Epoch 249/300\n",
      " - 0s - loss: 10.5423 - mse: 10.5423\n",
      "Epoch 250/300\n",
      " - 0s - loss: 10.5430 - mse: 10.5430\n",
      "Epoch 251/300\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-464313c46fa2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmlp_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m300\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m   1237\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1238\u001b[0m                                         \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1239\u001b[0;31m                                         validation_freq=validation_freq)\n\u001b[0m\u001b[1;32m   1240\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1241\u001b[0m     def evaluate(self,\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, fit_function, fit_inputs, out_labels, batch_size, epochs, verbose, callbacks, val_function, val_inputs, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq)\u001b[0m\n\u001b[1;32m    194\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 196\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfit_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    197\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/tensorflow_core/python/keras/backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   3738\u001b[0m         \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmath_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3739\u001b[0m       \u001b[0mconverted_inputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3740\u001b[0;31m     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_graph_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mconverted_inputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3741\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3742\u001b[0m     \u001b[0;31m# EagerTensor.numpy() will often make a copy to ensure memory safety.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1079\u001b[0m       \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mFor\u001b[0m \u001b[0minvalid\u001b[0m \u001b[0mpositional\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mkeyword\u001b[0m \u001b[0margument\u001b[0m \u001b[0mcombinations\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1080\u001b[0m     \"\"\"\n\u001b[0;32m-> 1081\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1082\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1083\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, args, kwargs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1119\u001b[0m       raise TypeError(\"Keyword arguments {} unknown. Expected {}.\".format(\n\u001b[1;32m   1120\u001b[0m           list(kwargs.keys()), list(self._arg_keywords)))\n\u001b[0;32m-> 1121\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1123\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1222\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1223\u001b[0m       flat_outputs = forward_function.call(\n\u001b[0;32m-> 1224\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager)\n\u001b[0m\u001b[1;32m   1225\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1226\u001b[0m       \u001b[0mgradient_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_delayed_rewrite_functions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregister\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    509\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    510\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"executor_type\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexecutor_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"config_proto\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 511\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    512\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    513\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/tensorflow_core/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tensorflow.TFE_Py_Execute(ctx._handle, device_name,\n\u001b[1;32m     60\u001b[0m                                                \u001b[0mop_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m                                                num_outputs)\n\u001b[0m\u001b[1;32m     62\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "history = mlp_model.fit(x_train, y_train, epochs=300, batch_size=32, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Printing graph of our training \n",
    "%matplotlib inline\n",
    "\n",
    "plt.plot(history.history['loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Falling back from v2 loop because of error: Failed to find data adapter that can handle input: <class 'pandas.core.frame.DataFrame'>, <class 'NoneType'>\n",
      "1253/1253 [==============================] - 0s 62us/sample - loss: 4.6842 - mse: 4.5878\n",
      "mse 4.587838\n"
     ]
    }
   ],
   "source": [
    "#Use the trained model for the dataset. End result is MSE = 4.950728\n",
    "#which is pretty good\n",
    "score = mlp_model.evaluate(x_test, y_test)\n",
    "print(mlp_model.metrics_names[1], score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.001\n"
     ]
    }
   ],
   "source": [
    "#Checking default learning rate\n",
    "import keras.backend as K\n",
    "\n",
    "print(K.eval(mlp_model.optimizer.lr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/charottamaoshmar/.local/lib/python3.7/site-packages/tensorflow_core/python/compat/v2_compat.py:65: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n"
     ]
    }
   ],
   "source": [
    "#self=implemented MLP\n",
    "#we need to use tensorflow v1 because tf.placeholder is deprecated\n",
    "#in tensorflow v2\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()\n",
    "\n",
    "#input neuron must be of same size to number of features column \n",
    "#hidden neurons can be any number, but output neuron is 1 because\n",
    "#this is a regression problem\n",
    "n_neurons_h = 8\n",
    "n_neurons_out = 1\n",
    "n_input = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#defining placeholder variables\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_input))\n",
    "Y = tf.placeholder(tf.float32, shape=(None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#weights and biases\n",
    "W1 = tf.get_variable(\"weights1\", dtype=tf.float32, initializer=tf.zeros((n_input, n_neurons_h)))\n",
    "b1 = tf.get_variable(\"bias1\", dtype=tf.float32, initializer=tf.zeros((n_neurons_h)))\n",
    "W2 = tf.get_variable(\"weights2\", dtype=tf.float32, initializer=tf.zeros((n_input, n_neurons_out)))\n",
    "b2 = tf.get_variable(\"bias2\", dtype=tf.float32, initializer=tf.zeros((n_neurons_out)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make the network\n",
    "#use relu as the activation function\n",
    "h = tf.nn.relu(tf.matmul(X, W1)+ b1)\n",
    "z = tf.matmul(h, W2) + b2\n",
    "#use MSE to measure the model’s performance\n",
    "#(the cost function)\n",
    "cost = tf.losses.mean_squared_error(labels=Y, predictions=z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 300\n",
    "\n",
    "#set the learning rate\n",
    "learning_rate = 0.01\n",
    "\n",
    "#create an optimizer by using the gradient descent optimization \n",
    "#function which is available in tensorflow module\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "\n",
    "#create a train operation by calling the minimise method of the optimizer to update the variables \n",
    "train_op = optimizer.minimize(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cost : 105.58313\n",
      "cost : 101.82592\n",
      "cost : 98.217354\n",
      "cost : 94.75157\n",
      "cost : 91.42261\n",
      "cost : 88.22583\n",
      "cost : 85.153854\n",
      "cost : 82.20502\n",
      "cost : 79.371864\n",
      "cost : 76.650925\n",
      "cost : 74.03779\n",
      "cost : 71.52757\n",
      "cost : 69.11674\n",
      "cost : 66.800934\n",
      "cost : 64.576935\n",
      "cost : 62.440723\n",
      "cost : 60.38899\n",
      "cost : 58.418194\n",
      "cost : 56.525215\n",
      "cost : 54.707344\n",
      "cost : 52.961002\n",
      "cost : 51.28365\n",
      "cost : 49.67273\n",
      "cost : 48.125305\n",
      "cost : 46.63916\n",
      "cost : 45.21154\n",
      "cost : 43.84051\n",
      "cost : 42.52345\n",
      "cost : 41.25844\n",
      "cost : 40.043404\n",
      "cost : 38.87625\n",
      "cost : 37.75529\n",
      "cost : 36.678658\n",
      "cost : 35.644444\n",
      "cost : 34.651237\n",
      "cost : 33.696938\n",
      "cost : 32.780437\n",
      "cost : 31.900112\n",
      "cost : 31.054487\n",
      "cost : 30.242222\n",
      "cost : 29.462206\n",
      "cost : 28.712788\n",
      "cost : 27.992914\n",
      "cost : 27.301653\n",
      "cost : 26.637432\n",
      "cost : 25.99951\n",
      "cost : 25.386765\n",
      "cost : 24.798183\n",
      "cost : 24.232779\n",
      "cost : 23.689672\n",
      "cost : 23.168007\n",
      "cost : 22.666899\n",
      "cost : 22.1856\n",
      "cost : 21.723198\n",
      "cost : 21.279089\n",
      "cost : 20.852442\n",
      "cost : 20.442581\n",
      "cost : 20.048803\n",
      "cost : 19.670687\n",
      "cost : 19.307518\n",
      "cost : 18.958569\n",
      "cost : 18.623316\n",
      "cost : 18.30124\n",
      "cost : 17.992014\n",
      "cost : 17.694847\n",
      "cost : 17.409399\n",
      "cost : 17.135216\n",
      "cost : 16.871817\n",
      "cost : 16.618748\n",
      "cost : 16.375698\n",
      "cost : 16.142157\n",
      "cost : 15.91784\n",
      "cost : 15.702351\n",
      "cost : 15.495292\n",
      "cost : 15.296437\n",
      "cost : 15.105348\n",
      "cost : 14.921807\n",
      "cost : 14.745467\n",
      "cost : 14.576026\n",
      "cost : 14.413283\n",
      "cost : 14.256952\n",
      "cost : 14.106755\n",
      "cost : 13.962393\n",
      "cost : 13.823821\n",
      "cost : 13.690586\n",
      "cost : 13.562623\n",
      "cost : 13.439664\n",
      "cost : 13.321565\n",
      "cost : 13.20811\n",
      "cost : 13.09905\n",
      "cost : 12.994301\n",
      "cost : 12.8936405\n",
      "cost : 12.796945\n",
      "cost : 12.704037\n",
      "cost : 12.614768\n",
      "cost : 12.528998\n",
      "cost : 12.446582\n",
      "cost : 12.367427\n",
      "cost : 12.291336\n",
      "cost : 12.218251\n",
      "cost : 12.148024\n",
      "cost : 12.080522\n",
      "cost : 12.015677\n",
      "cost : 11.953348\n",
      "cost : 11.893471\n",
      "cost : 11.835941\n",
      "cost : 11.780671\n",
      "cost : 11.727539\n",
      "cost : 11.676497\n",
      "cost : 11.627438\n",
      "cost : 11.580292\n",
      "cost : 11.534994\n",
      "cost : 11.491469\n",
      "cost : 11.449631\n",
      "cost : 11.409442\n",
      "cost : 11.370812\n",
      "cost : 11.333654\n",
      "cost : 11.297982\n",
      "cost : 11.263683\n",
      "cost : 11.230741\n",
      "cost : 11.199098\n",
      "cost : 11.168616\n",
      "cost : 11.13936\n",
      "cost : 11.111265\n",
      "cost : 11.084232\n",
      "cost : 11.058273\n",
      "cost : 11.033307\n",
      "cost : 11.009298\n",
      "cost : 10.986227\n",
      "cost : 10.96406\n",
      "cost : 10.942773\n",
      "cost : 10.922274\n",
      "cost : 10.902577\n",
      "cost : 10.883643\n",
      "cost : 10.865464\n",
      "cost : 10.847995\n",
      "cost : 10.831152\n",
      "cost : 10.814998\n",
      "cost : 10.799438\n",
      "cost : 10.784516\n",
      "cost : 10.770147\n",
      "cost : 10.756341\n",
      "cost : 10.743054\n",
      "cost : 10.730298\n",
      "cost : 10.718019\n",
      "cost : 10.706222\n",
      "cost : 10.694915\n",
      "cost : 10.6839695\n",
      "cost : 10.673477\n",
      "cost : 10.663412\n",
      "cost : 10.653716\n",
      "cost : 10.644379\n",
      "cost : 10.635414\n",
      "cost : 10.626804\n",
      "cost : 10.618478\n",
      "cost : 10.610513\n",
      "cost : 10.6028385\n",
      "cost : 10.595444\n",
      "cost : 10.588344\n",
      "cost : 10.5815325\n",
      "cost : 10.574976\n",
      "cost : 10.568655\n",
      "cost : 10.562612\n",
      "cost : 10.556728\n",
      "cost : 10.551094\n",
      "cost : 10.545735\n",
      "cost : 10.540521\n",
      "cost : 10.535504\n",
      "cost : 10.530691\n",
      "cost : 10.526053\n",
      "cost : 10.521615\n",
      "cost : 10.5173\n",
      "cost : 10.513205\n",
      "cost : 10.509222\n",
      "cost : 10.505397\n",
      "cost : 10.501732\n",
      "cost : 10.498237\n",
      "cost : 10.494776\n",
      "cost : 10.491498\n",
      "cost : 10.488349\n",
      "cost : 10.485315\n",
      "cost : 10.482377\n",
      "cost : 10.479581\n",
      "cost : 10.476833\n",
      "cost : 10.474267\n",
      "cost : 10.471747\n",
      "cost : 10.469328\n",
      "cost : 10.467025\n",
      "cost : 10.464747\n",
      "cost : 10.462627\n",
      "cost : 10.460544\n",
      "cost : 10.458549\n",
      "cost : 10.456644\n",
      "cost : 10.454759\n",
      "cost : 10.452965\n",
      "cost : 10.451247\n",
      "cost : 10.449608\n",
      "cost : 10.448011\n",
      "cost : 10.446456\n",
      "cost : 10.445003\n",
      "cost : 10.443575\n",
      "cost : 10.442213\n",
      "cost : 10.440874\n",
      "cost : 10.4396105\n",
      "cost : 10.438364\n",
      "cost : 10.437181\n",
      "cost : 10.43605\n",
      "cost : 10.434961\n",
      "cost : 10.43388\n",
      "cost : 10.432866\n",
      "cost : 10.431861\n",
      "cost : 10.430898\n",
      "cost : 10.42996\n",
      "cost : 10.429126\n",
      "cost : 10.428304\n",
      "cost : 10.427472\n",
      "cost : 10.426675\n",
      "cost : 10.425915\n",
      "cost : 10.425175\n",
      "cost : 10.424474\n",
      "cost : 10.423829\n",
      "cost : 10.42312\n",
      "cost : 10.422471\n",
      "cost : 10.421866\n",
      "cost : 10.421261\n",
      "cost : 10.42067\n",
      "cost : 10.420159\n",
      "cost : 10.419634\n",
      "cost : 10.419107\n",
      "cost : 10.418599\n",
      "cost : 10.418144\n",
      "cost : 10.417674\n",
      "cost : 10.417233\n",
      "cost : 10.416829\n",
      "cost : 10.4163685\n",
      "cost : 10.415975\n",
      "cost : 10.415586\n",
      "cost : 10.415186\n",
      "cost : 10.414838\n",
      "cost : 10.414487\n",
      "cost : 10.414171\n",
      "cost : 10.413841\n",
      "cost : 10.413522\n",
      "cost : 10.413204\n",
      "cost : 10.412917\n",
      "cost : 10.412647\n",
      "cost : 10.412377\n",
      "cost : 10.4121065\n",
      "cost : 10.411805\n",
      "cost : 10.411583\n",
      "cost : 10.4113455\n",
      "cost : 10.411114\n",
      "cost : 10.41089\n",
      "cost : 10.410668\n",
      "cost : 10.410469\n",
      "cost : 10.410252\n",
      "cost : 10.41006\n",
      "cost : 10.409866\n",
      "cost : 10.409716\n",
      "cost : 10.409503\n",
      "cost : 10.40933\n",
      "cost : 10.40919\n",
      "cost : 10.408988\n",
      "cost : 10.408833\n",
      "cost : 10.408701\n",
      "cost : 10.408577\n",
      "cost : 10.408444\n",
      "cost : 10.4083\n",
      "cost : 10.408154\n",
      "cost : 10.408006\n",
      "cost : 10.407904\n",
      "cost : 10.407777\n",
      "cost : 10.4076605\n",
      "cost : 10.407541\n",
      "cost : 10.407423\n",
      "cost : 10.407336\n",
      "cost : 10.407232\n",
      "cost : 10.407121\n",
      "cost : 10.407023\n",
      "cost : 10.406944\n",
      "cost : 10.406845\n",
      "cost : 10.406773\n",
      "cost : 10.406703\n",
      "cost : 10.40658\n",
      "cost : 10.406496\n",
      "cost : 10.406438\n",
      "cost : 10.406334\n",
      "cost : 10.40627\n",
      "cost : 10.4062195\n",
      "cost : 10.406124\n",
      "cost : 10.40597\n",
      "cost : 10.406017\n",
      "cost : 10.40595\n",
      "cost : 10.405875\n",
      "cost : 10.405823\n",
      "cost : 10.405733\n",
      "cost : 10.405669\n",
      "cost : 10.405673\n",
      "cost : 10.40565\n",
      "cost : 10.405538\n"
     ]
    }
   ],
   "source": [
    "#initialize the variables and execute training \n",
    "init = tf.global_variables_initializer()\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for epoch in range (n_epochs):\n",
    "        sess.run(train_op, feed_dict = {X: x_train, Y:y_train})\n",
    "        print('cost :',sess.run(cost, feed_dict={X:x_test,Y:y_test}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "105.73243\n",
      "102.11624\n",
      "98.64001\n",
      "95.29865\n",
      "92.08641\n",
      "88.99864\n",
      "86.030426\n",
      "83.176735\n",
      "80.43358\n",
      "77.796646\n",
      "75.26131\n",
      "72.82371\n",
      "70.48036\n",
      "68.22717\n",
      "66.061264\n",
      "63.978172\n",
      "61.97549\n",
      "60.050148\n",
      "58.198433\n",
      "56.41825\n",
      "54.706345\n",
      "53.060158\n",
      "51.477123\n",
      "49.954746\n",
      "48.49072\n",
      "47.082787\n",
      "45.72868\n",
      "44.426456\n",
      "43.17395\n",
      "41.96931\n",
      "40.81066\n",
      "39.696087\n",
      "38.624054\n",
      "37.59293\n",
      "36.60092\n",
      "35.646786\n",
      "34.7288\n",
      "33.845657\n",
      "32.99607\n",
      "32.178802\n",
      "31.392029\n",
      "30.63559\n",
      "29.907427\n",
      "29.207018\n",
      "28.532814\n",
      "27.884052\n",
      "27.259836\n",
      "26.659088\n",
      "26.080893\n",
      "25.524424\n",
      "24.988876\n",
      "24.473389\n",
      "23.977215\n",
      "23.49958\n",
      "23.039827\n",
      "22.597284\n",
      "22.17115\n",
      "21.76089\n",
      "21.365961\n",
      "20.98567\n",
      "20.619497\n",
      "20.266926\n",
      "19.927399\n",
      "19.600454\n",
      "19.28557\n",
      "18.982384\n",
      "18.69027\n",
      "18.408766\n",
      "18.137825\n",
      "17.876747\n",
      "17.6253\n",
      "17.382948\n",
      "17.14956\n",
      "16.924658\n",
      "16.707968\n",
      "16.499105\n",
      "16.297886\n",
      "16.103981\n",
      "15.917052\n",
      "15.736898\n",
      "15.563261\n",
      "15.395853\n",
      "15.234609\n",
      "15.078901\n",
      "14.9289875\n",
      "14.784382\n",
      "14.644948\n",
      "14.510469\n",
      "14.380844\n",
      "14.255734\n",
      "14.1351385\n",
      "14.018776\n",
      "13.906552\n",
      "13.798279\n",
      "13.693814\n",
      "13.593021\n",
      "13.495749\n",
      "13.401989\n",
      "13.311363\n",
      "13.223965\n",
      "13.139572\n",
      "13.058161\n",
      "12.979553\n",
      "12.903649\n",
      "12.830366\n",
      "12.75963\n",
      "12.691247\n",
      "12.625262\n",
      "12.561583\n",
      "12.500021\n",
      "12.44056\n",
      "12.383119\n",
      "12.327652\n",
      "12.274009\n",
      "12.222198\n",
      "12.1721115\n",
      "12.123821\n",
      "12.077\n",
      "12.031757\n",
      "11.988051\n",
      "11.945796\n",
      "11.9049225\n",
      "11.86544\n",
      "11.82723\n",
      "11.790261\n",
      "11.754534\n",
      "11.719921\n",
      "11.686482\n",
      "11.65415\n",
      "11.622779\n",
      "11.592521\n",
      "11.563196\n",
      "11.534808\n",
      "11.507336\n",
      "11.480721\n",
      "11.454953\n",
      "11.430029\n",
      "11.405884\n",
      "11.382564\n",
      "11.3598385\n",
      "11.337892\n",
      "11.316633\n",
      "11.295998\n",
      "11.276096\n",
      "11.256801\n",
      "11.2380295\n",
      "11.21987\n",
      "11.202244\n",
      "11.185182\n",
      "11.168612\n",
      "11.152592\n",
      "11.137006\n",
      "11.121943\n",
      "11.107298\n",
      "11.093073\n",
      "11.079354\n",
      "11.065969\n",
      "11.0529995\n",
      "11.040407\n",
      "11.028195\n",
      "11.016344\n",
      "11.00481\n",
      "10.99367\n",
      "10.982827\n",
      "10.972347\n",
      "10.962042\n",
      "10.952121\n",
      "10.9424925\n",
      "10.933114\n",
      "10.924014\n",
      "10.915162\n",
      "10.9065695\n",
      "10.898192\n",
      "10.8901\n",
      "10.882237\n",
      "10.874539\n",
      "10.867074\n",
      "10.859839\n",
      "10.852768\n",
      "10.8459\n",
      "10.8392935\n",
      "10.832742\n",
      "10.8264675\n",
      "10.8203125\n",
      "10.814328\n",
      "10.808488\n",
      "10.802865\n",
      "10.797405\n",
      "10.792012\n",
      "10.786795\n",
      "10.781714\n",
      "10.776836\n",
      "10.771966\n",
      "10.767326\n",
      "10.762723\n",
      "10.758283\n",
      "10.753943\n",
      "10.749752\n",
      "10.745638\n",
      "10.741625\n",
      "10.737754\n",
      "10.733944\n",
      "10.730245\n",
      "10.726639\n",
      "10.723103\n",
      "10.719744\n",
      "10.716337\n",
      "10.713085\n",
      "10.709939\n",
      "10.706811\n",
      "10.70381\n",
      "10.700853\n",
      "10.698022\n",
      "10.695175\n",
      "10.692491\n",
      "10.689809\n",
      "10.687219\n",
      "10.684664\n",
      "10.682189\n",
      "10.679756\n",
      "10.677433\n",
      "10.67514\n",
      "10.672963\n",
      "10.670724\n",
      "10.668551\n",
      "10.666476\n",
      "10.664438\n",
      "10.66247\n",
      "10.66051\n",
      "10.658644\n",
      "10.65679\n",
      "10.654981\n",
      "10.653255\n",
      "10.651485\n",
      "10.649766\n",
      "10.648182\n",
      "10.646568\n",
      "10.644972\n",
      "10.643471\n",
      "10.64197\n",
      "10.640497\n",
      "10.639095\n",
      "10.6376915\n",
      "10.636315\n",
      "10.63498\n",
      "10.633695\n",
      "10.632381\n",
      "10.631187\n",
      "10.6299515\n",
      "10.628766\n",
      "10.6276245\n",
      "10.626473\n",
      "10.625354\n",
      "10.624262\n",
      "10.623209\n",
      "10.6221695\n",
      "10.6211815\n",
      "10.620195\n",
      "10.6192255\n",
      "10.618288\n",
      "10.617325\n",
      "10.616427\n",
      "10.615529\n",
      "10.614669\n",
      "10.613899\n",
      "10.612991\n",
      "10.612184\n",
      "10.611408\n",
      "10.610617\n",
      "10.60986\n",
      "10.609114\n",
      "10.608394\n",
      "10.607678\n",
      "10.606982\n",
      "10.606299\n",
      "10.605643\n",
      "10.604986\n",
      "10.604366\n",
      "10.6037245\n",
      "10.603098\n",
      "10.602505\n",
      "10.601929\n",
      "10.601363\n",
      "10.600793\n",
      "10.600268\n",
      "10.599743\n",
      "10.5992155\n",
      "10.598663\n",
      "10.598144\n",
      "10.59768\n",
      "10.597234\n",
      "10.596735\n",
      "10.596287\n",
      "10.595843\n",
      "10.595399\n",
      "10.594955\n",
      "10.594521\n",
      "10.594108\n",
      "10.593716\n",
      "10.593318\n"
     ]
    }
   ],
   "source": [
    "n_records = x_train.shape\n",
    "\n",
    "#set the batch size\n",
    "batch_size = 100\n",
    "\n",
    "#random choose the index in the total 2923 records\n",
    "rand_index = np.random.choice(2923, size=batch_size)\n",
    "\n",
    "#get the training values in these index\n",
    "x_batch = x_train.values[rand_index, :]\n",
    "y_batch = y_train.values[rand_index]\n",
    "y_batch\n",
    "\n",
    "#execute the validation with the batch size implemented \n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for epoch in range(n_epochs):\n",
    "        sess.run(train_op, feed_dict = {X: x_batch, Y:y_batch})\n",
    "        print(sess.run(cost, feed_dict={X:\n",
    "        x_test, Y:y_test}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
